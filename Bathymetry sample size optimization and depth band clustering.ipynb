{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample size optimization and impact on clustering experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large-scale (e.g., continental to global scale) structural partitioning of ocean depths to isolate world's continental shelves. Shelf attributes/defining characteristics include:\n",
    "\n",
    "- slopes < 0.69 degrees\n",
    "- depths < -1,211 meters\n",
    "\n",
    "(1) based on Wright et al. (2001)\n",
    "(2) based on optimal Fisher-Jenks Natural Breaks clusters for all ocean depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above proposed model, I'm confident, is approaching optimum, however, more validation (quantitative preferred, but qualtative support will do) is needed to provide a more cogent argument. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering has merit for helping to isolate continental shelf environments, but the 'fly-in-the-oinment' is the non-convex, mixed-density distribution of depths across the global oceans. Because of this variation, both density and distance algorithms return mixed, and so questionable, results. \n",
    "\n",
    "One problem with the results thus far might be associated with the size of the sample being fed into the clustering algorithms. To date, we've been pouring samples on the order of 100,000 to 500,000 points into k-means and DBSCAN. The returns have been less than definitive, or revealing. Perhaps though, it is this large sample size, used to ensure a representative sampling of the ocean bathymetry, that the problem lies. \n",
    "\n",
    "Hypothesis: Consider that the larger the sample count the more representative the sampling, but moreover, the higher the resolution, and thus the more finer detail is captured and preserved. Further, as sample sizes increase, the concomitant increase in resolution masks the global structure in a growing collection of finer and finer detail. Thus, it might well be that by increasing sampling sizes, detectable evidence for the global structure becomes more and more difficult to isolate. \n",
    "\n",
    "It is the global structure that we're interested in--the continental shelves represent a group of globally distributed subaqueous surfaces that are of relative shallow depth and gentle slopes. \n",
    "\n",
    "If this hypothesis is indeed true, then it by using large sample sizes, it is difficult (intractible) to isolate continental shelf environments from all others in our data.\n",
    "\n",
    "Thus, the time has come to reduce sample sizes to see if some sort of global structure emerges in plain view of the clustering algorithms being used to identify shelves..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An earlier computed minimum sample size for the ETOPO1_bathy_1km data model:\n",
    "\n",
    "$ \\delta = t s/n^{0.5}$\n",
    "\n",
    "or\n",
    "\n",
    "$ n = (ts/\\delta)^2 $\n",
    "\n",
    "$ n = (1.65*1998.94/10)^2 $\n",
    "\n",
    "$ n = $ **108,784.6** or about **109,000** observations (minimum) required to provide sufficent power for parametric testing and comparison. Note, however, that we are doing no parametric testing--only clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lack of external guidance on a starting point for a sample sufficiently small to present the global structure, let's begin with a sample size of $0.1n$:\n",
    "\n",
    "n = 10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************************\n",
    "For 10000 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in GRASS:\n",
    "    \n",
    "g.region ETOPO1_world_1km\n",
    "\n",
    "r.random --o input=ETOPO1_bathy_1211m_069deg_1km npoints=10000 vector=ETOPO1_10ksamplepts\n",
    "\n",
    "v.out.ascii input=ETOPO1_10ksamplepts output=/Users/paulparis/Documents/Projects/csi/data/table/ETOPO1_10ksamplepts.csv columns=value format=point separator=comma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Fisher-Jenks Natural Breaks Clustering algorithm w/GVF=0.90:\n",
    "\n",
    "For n=10,000\n",
    " Lower              Upper                Count <br>\n",
    "           x[i] <= -784.407                532 <br>\n",
    "-784.407 < x[i] <= -515.793                667 <br>\n",
    "-515.793 < x[i] <= -332.698                767 <br>\n",
    "-332.698 < x[i] <= -188.668               1036 <br>\n",
    "-188.668 < x[i] <=  -91.787               1856 <br>\n",
    " -91.787 < x[i] <=  -36.647               2458 <br>\n",
    " -36.647 < x[i] <=   -0.005               2684 <br>\n",
    " \n",
    "Using the Fisher-Jenks Natural Breaks Clustering algorithm w/GVF=0.80\n",
    "('Count of observations in each class:', array([ 590,  932, 1300, 2661, 4517]))\n",
    "Class upper bounds:  -743.00\n",
    "Class upper bounds:  -425.98\n",
    "Class upper bounds:  -206.19\n",
    "Class upper bounds:   -73.43\n",
    "Class upper bounds:    -0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 5000 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in GRASS:\n",
    "    \n",
    "r.random --o input=ETOPO1_bathy_1211m_069deg_1km npoints=5000 vector=ETOPO1_5ksamplepts\n",
    "\n",
    "v.out.ascii input=ETOPO1_5ksamplepts output=/Users/paulparis/Documents/Projects/csi/data/table/ETOPO1_5ksamplepts.csv columns=value format=point separator=comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Using the Fisher-Jenks Natural Breaks Clustering algorithm w/GVF=0.80:\n",
    "\n",
    "For 5000 observations:\n",
    "('Count of observations in each class:', array([ 590,  932, 1300, 2661, 4517]))\n",
    "('Count of observations in each class:', array([ 271,  510,  732, 1321, 2166]))\n",
    "Class upper bounds:  -735.55\n",
    "Class upper bounds:  -407.57\n",
    "Class upper bounds:  -189.17\n",
    "Class upper bounds:   -65.23\n",
    "Class upper bounds:    -0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 3750 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in GRASS:\n",
    "    \n",
    "r.random --o input=ETOPO1_bathy_1211m_069deg_1km npoints=3750 vector=ETOPO1_3750samplepts\n",
    "\n",
    "v.out.ascii input=ETOPO1_3750samplepts output=/Users/paulparis/Documents/Projects/csi/data/table/ETOPO1_3750samplepts.csv columns=value format=point separator=comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Using the Fisher-Jenks Natural Breaks Clustering algorithm w/GVF=0.80:\n",
    "\n",
    "For 3750 observations:\n",
    "('Count of observations in each class:', array([ 210,  366,  561,  975, 1638]))\n",
    "Class upper bounds:  -734.49\n",
    "Class upper bounds:  -399.89\n",
    "Class upper bounds:  -186.57\n",
    "Class upper bounds:   -66.57\n",
    "Class upper bounds:    -0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2500 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in GRASS:\n",
    "\n",
    "r.random --o input=ETOPO1_bathy_1211m_069deg_1km npoints=2500 vector=ETOPO1_2500samplepts\n",
    "\n",
    "v.out.ascii input=ETOPO1_2500samplepts output=/Users/paulparis/Documents/Projects/csi/data/table/ETOPO1_2500samplepts.csv columns=value format=point separator=comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Using the Fisher-Jenks Natural Breaks Clustering algorithm w/GVF=0.80:\n",
    "\n",
    "For 2500 observations:\n",
    "('Count of observations in each class:', array([ 136,  245,  312,  646, 1161]))\n",
    "Class upper bounds:  -789.50\n",
    "Class upper bounds:  -441.85\n",
    "Class upper bounds:  -212.59\n",
    "Class upper bounds:   -74.71\n",
    "Class upper bounds:    -0.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1000 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in GRASS:\n",
    "\n",
    "r.random --o input=ETOPO1_bathy_1211m_069deg_1km npoints=1000 vector=ETOPO1_1000samplepts\n",
    "\n",
    "v.out.ascii input=ETOPO1_1000samplepts output=/Users/paulparis/Documents/Projects/csi/data/table/ETOPO1_1000samplepts.csv columns=value format=point separator=comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Using the Fisher-Jenks Natural Breaks Clustering algorithm w/GVF=0.80:\n",
    "\n",
    "For 1000 observations:\n",
    "('Count of observations in each class:', array([ 71, 114, 123, 248, 444]))\n",
    "Class upper bounds:  -648.65\n",
    "Class upper bounds:  -349.80\n",
    "Class upper bounds:  -170.71\n",
    "Class upper bounds:   -65.06\n",
    "Class upper bounds:    -0.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on results thus far, samples with observations ranging from 10000 down to 1000 sample points yield similar results. Moreover, the outcomes from the 5000 and 3750 count samples are strikingly close--much closer than seen between 3750 and 3500 sample points, though the differences in observation counts are the same (1250). \n",
    "\n",
    "To get a better picture of what's happening, and to better ascertain any possible meaning in the convergence seen in the data between 5000 and 3750, it would be worth trying additional samples at 7,500, 12500, and 15000. Does the 3750 - 5000 observation region mark a/the 'sweet-spot' for best revealing the large-scale (global) structure of the depth data?\n",
    "\n",
    "For 7500 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in GRASS:\n",
    "\n",
    "r.random --o input=ETOPO1_bathy_1211m_069deg_1km npoints=7500 vector=ETOPO1_7500samplepts\n",
    "\n",
    "v.out.ascii input=ETOPO1_7500samplepts output=/Users/paulparis/Documents/Projects/csi/data/table/ETOPO1_7500samplepts.csv columns=value format=point separator=comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Using the Fisher-Jenks Natural Breaks Clustering algorithm w/GVF=0.80:\n",
    "\n",
    "For 7500 observations:\n",
    "('Count of observations in each class:', array([ 423,  767,  980, 1856, 3474]))\n",
    "Class upper bounds:  -748.06\n",
    "Class upper bounds:  -416.49\n",
    "Class upper bounds:  -203.69\n",
    "Class upper bounds:   -73.50\n",
    "Class upper bounds:    -0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 12500 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in GRASS:\n",
    "\n",
    "r.random --o input=ETOPO1_bathy_1211m_069deg_1km npoints=12500 vector=ETOPO1_12500samplepts\n",
    "\n",
    "v.out.ascii input=ETOPO1_12500samplepts output=/Users/paulparis/Documents/Projects/csi/data/table/ETOPO1_12500samplepts.csv columns=value format=point separator=comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Using the Fisher-Jenks Natural Breaks Clustering algorithm w/GVF=0.80:\n",
    "\n",
    "For 12500 observations:\n",
    "('Count of observations in each class:', array([ 812, 1195, 1663, 3358, 5472]))\n",
    "Class upper bounds:  -737.47\n",
    "Class upper bounds:  -417.03\n",
    "Class upper bounds:  -199.34\n",
    "Class upper bounds:   -69.17\n",
    "Class upper bounds:    -0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 15000 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in GRASS:\n",
    "\n",
    "r.random --o input=ETOPO1_bathy_1211m_069deg_1km npoints=15000 vector=ETOPO1_15ksamplepts\n",
    "\n",
    "v.out.ascii input=ETOPO1_15ksamplepts output=/Users/paulparis/Documents/Projects/csi/data/table/ETOPO1_15ksamplepts.csv columns=value format=point separator=comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Using the Fisher-Jenks Natural Breaks Clustering algorithm w/GVF=0.80:\n",
    "\n",
    "For 15000 observations:\n",
    "('Count of observations in each class:', array([ 802, 1533, 2146, 3798, 6721]))\n",
    "Class upper bounds:  -754.15\n",
    "Class upper bounds:  -406.85\n",
    "Class upper bounds:  -188.35\n",
    "Class upper bounds:   -67.98\n",
    "Class upper bounds:    -0.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.4/site-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fpath='/Users/paulparis/Documents/Projects/csi/data/table/'\n",
    "fname='ETOPO1_3750samplepts.csv'\n",
    "\n",
    "df=pandas.read_csv(fpath+fname, sep=',', skiprows=0, names=['x','y','id','z'])\n",
    "\n",
    "# extract x,y,and z from the df\n",
    "x=df.as_matrix(columns=['x'])\n",
    "y=df.as_matrix(columns=['y'])\n",
    "z=df.as_matrix(columns=['z'])\n",
    "\n",
    "# set up the class colors\n",
    "clrs = []\n",
    "\n",
    "classes = [-65.23,-189.17,-407.57,-735.55]\n",
    "for v in df['z']:\n",
    "    if( v >= -65.0):\n",
    "        clrs.append((0.0, 0.75, 0.75))\n",
    "    if( v < -65.0 and v >= -189.0):\n",
    "        clrs.append((0.75, 0.75, 0))    \n",
    "    if( v < -189.0 and v >= -407.0):\n",
    "        clrs.append((0.0, 0.5, 0.0) )\n",
    "    if( v < -407.0 and v >= -735.0):\n",
    "        clrs.append((1.0, 0, 0.75))\n",
    "    if( v < -735.0):\n",
    "        clrs.append((1.0, 1.0, 1.0))\n",
    "       \n",
    "# plot the clustered data\n",
    "fignum = 1\n",
    "fig = plt.figure(fignum, figsize=(7,7))\n",
    "plt.clf()   # clear the figure\n",
    "# \n",
    "ax = Axes3D(fig, rect=[0,0,0.95,1], elev=40, azim=134 )\n",
    "plt.cla()\n",
    "\n",
    "ax.scatter(x,y,z, c=clrs) \n",
    "\t#ax.set_zlim(-8000,0)\n",
    "\t#ax.set_ylim(0,8000000)\n",
    "ax.set_xlabel=('Slope (degrees)')\n",
    "ax.set_ylabel=('Latitude (DD)')\n",
    "ax.set_zlabel=('Depth (meters)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(numpy.shape(z) )\n",
    "df.isnull().any().any()\n",
    "df.isnull().sum().sum()\n",
    "###df.replace({'\\n': '<br>'}, regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plot and examine the kernel density estimate(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# import data into pandas dataframe\n",
    "fpath='/Users/paulparis/Documents/Projects/csi/data/table/'\n",
    "fname='ETOPO1_5ksamplepts.csv'\n",
    "\n",
    "df=pandas.read_csv(fpath+fname, sep=',', skiprows=0, names=['x','y','id','z'])\n",
    "\n",
    "# extract x,y,and z from the df\n",
    "x=df.as_matrix(columns=['x'])\n",
    "y=df.as_matrix(columns=['y'])\n",
    "z=df.as_matrix(columns=['z'])\n",
    "\n",
    "\n",
    "# generate the grid\n",
    "z_grid = np.linspace( 0, -1200, num=255)\n",
    "\n",
    "kde = KernelDensity( kernel='gaussian', bandwidth=1.0)\n",
    "kde.fit(z)\n",
    "# since the kde fit is returned as the natural log\n",
    "pdf = np.exp( kde.score_samples(z_grid[ :,np.newaxis]) )\n",
    "#pdf = kde.score_samples(z_grid[ :,np.newaxis])\n",
    "\n",
    "# cross-validate to check for approapriate/optimal bandwidth selection\n",
    "#grid = GridSearchCV(KernelDensity(), {'bandwidth': np.linspace(0.1, 1.0, 30)}, cv=20)\n",
    "#grid.fit(z)\n",
    "#print(grid.best_params_)\n",
    "\n",
    "fignum = 2\n",
    "fig2 = plt.figure(fignum, figsize=(7,7))\n",
    "plt.clf()   # clear the figure\n",
    "\n",
    "ax = fig2.add_subplot(1,1,1)\n",
    "\n",
    "ax.plot(z_grid, pdf, color='blue', alpha=0.5, lw=1.5, label='KDE (3750 random obs.)')\n",
    "\n",
    "# major ticks every 20, minor ticks every 5                                      \n",
    "major_ticks = np.arange(-1200, 0, 50)                                              \n",
    "#minor_ticks = np.arange(-1200, 101, 5)                                               \n",
    "\n",
    "ax.set_xticks(major_ticks)                                                       \n",
    "#ax.set_xticks(minor_ticks, minor=True)                                           \n",
    "#ax.set_yticks(major_ticks)                                                       \n",
    "#ax.set_yticks(minor_ticks, minor=True)                                           \n",
    "\n",
    "# and a corresponding grid                                                       \n",
    "\n",
    "ax.grid(which='both')                                                            \n",
    "\n",
    "# or if you want differnet settings for the grids:                               \n",
    "ax.grid(which='minor', alpha=0.2)                                                \n",
    "ax.grid(which='major', alpha=0.5)                  \n",
    "\n",
    "ax.set_xlabel('Water Depth (meters)' )\n",
    "ax.set_ylabel('Depth Model Density Estimate (KDE)')\n",
    "ax.set_title('ETOPO1 Global Shelf Waters - Kernel (Probability) Density Estimates' )\n",
    "#ax.grid(True)\n",
    "ax.legend(loc='best')\n",
    "plt.show()\n",
    "#fig.savefig( '/Users/paulparis/Desktop/cb_kde_zone1.pdf' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the validity of the results for the smaller random samples (<=15000) try a slightly larger sample. This might give us some idea of whether or not the smaller samples are converging to optimum, or that it really doesn't matter too much whether the size of the sample. The variation due to random differences (residuals) deviate only due to random sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 25000 sample points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in GRASS:\n",
    "\n",
    "r.random --o input=ETOPO1_bathy_1211m_069deg_1km npoints=25000 vector=ETOPO1_25ksamplepts\n",
    "\n",
    "v.out.ascii input=ETOPO1_25ksamplepts output=/Users/paulparis/Documents/Projects/csi/data/table/ETOPO1_25ksamplepts.csv columns=value format=point separator=comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Using the Fisher-Jenks Natural Breaks Clustering algorithm w/GVF=0.80:\n",
    "\n",
    "For 25000 observations:\n",
    "('Count of observations in each class:', array([ 1385,  2328,  3091,  6437, 11759]))\n",
    "Class upper bounds:  -748.39\n",
    "Class upper bounds:  -426.34\n",
    "Class upper bounds:  -208.92\n",
    "Class upper bounds:   -73.81\n",
    "Class upper bounds:    -0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a preliminary shallow shelf (0 to -188 meters) DEM for sample testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In GRASS:\n",
    "\n",
    "Shallow Shelf Model (0 to -188.0 meters deep)\n",
    "r.mapcalc --o\n",
    "ETOPO1_bathy_shelf_0_188m_1km = if(ETOPO1_bathy_1211m_069deg_1km <=0 && ETOPO1_bathy_1211m_069deg_1km > -188.0, ETOPO1_bathy_1211m_069deg_1km, null() )\n",
    "\n",
    "Intermediate Shelf Model (-188.0 to -735.0 meters deep)\n",
    "r.mapcalc --o\n",
    "ETOPO1_bathy_shelf_188_735m_1km = if(ETOPO1_bathy_1211m_069deg_1km <=-188.0 && ETOPO1_bathy_1211m_069deg_1km > -735.0, ETOPO1_bathy_1211m_069deg_1km, null() )\n",
    "\n",
    "Deep Shelf Model (< -735.0 meters deep)\n",
    "r.mapcalc --o\n",
    "ETOPO1_bathy_shelf_sub735m_1km = if(ETOPO1_bathy_1211m_069deg_1km <=-735, ETOPO1_bathy_1211m_069deg_1km, null() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 7 km wide buffers around digitized sample transects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in GRASS:\n",
    "    \n",
    "v.buffer -c -t input=ETOPO1_NA_sampletransects output=ETOPO1_NA_samplebuffers distance=3500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture shelf (shallow,intermediate, deep) character under transects and in buffers:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In GRASS:\n",
    "    \n",
    "For sampletransects:\n",
    "v.rast.stats --verbose map=ETOPO1_NA_sampletransects raster=ETOPO1_bathy_shelf_0_188m_1km column_prefix=s_ method=minimum,maximum,average,stddev\n",
    "\n",
    "v.rast.stats --verbose map=ETOPO1_NA_sampletransects raster=ETOPO1_bathy_shelf_188_735m_1km column_prefix=i_ method=minimum,maximum,average,stddev\n",
    "\n",
    "v.rast.stats --verbose map=ETOPO1_NA_sampletransects raster=ETOPO1_bathy_shelf_sub735m_1km column_prefix=d_ method=minimum,maximum,average,stddev\n",
    "\n",
    "\n",
    "For samplebuffers:\n",
    "v.rast.stats --verbose map=ETOPO1_NA_samplebuffers raster=ETOPO1_bathy_shelf_0_188m_1km column_prefix=s_ method=minimum,maximum,average,stddev\n",
    "\n",
    "v.rast.stats --verbose map=ETOPO1_NA_samplebuffers raster=ETOPO1_bathy_shelf_188_735m_1km column_prefix=i_ method=minimum,maximum,average,stddev\n",
    "\n",
    "v.rast.stats --verbose map=ETOPO1_NA_samplebuffers raster=ETOPO1_bathy_shelf_sub735m_1km column_prefix=d_ method=minimum,maximum,average,stddev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the transects and buffers (now populated with DEM attributes) to ESRI shapefiles..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To generate the final shelf class boundaries (shallow, shallow-intermediate, deep-intermediate, deep) we will draw a random sample of depths from ETOPO1_bathy_1211m_069_1km of 10,000 observations. From this sample, 1,000 subsamples will be \"bootstrapped\" and used to generate Fisher-Jenks Natural Breaks classes. With each iteration, the class boundaries will be collected. In the end, or maybe along the way, these boundaries will be summed and a mean for each class computed. These means will be the final class boundaries reported in the study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Generate the 10,000 observation random sample from ETOPO1\n",
    "\n",
    "Fortunately, this was created earlier as part of the prototyping and testing, so ETOPO1_10ksamplepts is already in hand.\n",
    "\n",
    "path: /Users/paulparis/Documents/Projects/csi/dta/table/ <br>\n",
    "file: ETOPO1_10ksamplepts.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) create a script in Python to interrogate ETOPO1_10ksamplepts.csv 1000 times, each time extracting a random subsample of 5,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "--- 9855.69165397 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "#import pandas\n",
    "from pysal.esda.mapclassify import Natural_Breaks as nb\n",
    "import time\n",
    "\n",
    "def LoadSrcDataToArray(infilepath, infilename):\n",
    "    '''\n",
    "    opens the source file and loads the values (column) to be classified, to array\n",
    "    '''\n",
    "    fp=open(infilepath+infilename, 'r')\n",
    "    \n",
    "    value_list=[]\n",
    "    for line in fp:\n",
    "        _,_,_,value=line.rstrip().split(',')   # ADJUST THIS LINE TO MATCH INPUT DATA\n",
    "        value_list.append(float(value))\n",
    "        \n",
    "    fp.close()\n",
    "    \n",
    "    return(np.array(value_list))\n",
    "\n",
    "def ComputeNaturalBreaks(pc_array, klasses):\n",
    "    '''\n",
    "    partitions data into classes using PySAL pysal.esda.mapclassify, Natural_Breaks algorithm. \n",
    "    input: array of values to classify\n",
    "    returns: upper bounds for classes\n",
    "    '''\n",
    "    breaks=nb(pc_array, k=klasses, initial=20)\n",
    "    return(breaks)\n",
    "\n",
    "\n",
    "def goodness_of_variance_fit(array, classes):\n",
    "    # get the break points\n",
    "    #classes = jenks(array, classes)\n",
    "\n",
    "    # do the actual classification - assign values based on class breaks\n",
    "    classified = np.array([classify(i, classes) for i in array])\n",
    "\n",
    "    # max value of zones\n",
    "    maxz = max(classified)\n",
    "\n",
    "    # nested list of zone indices\n",
    "    zone_indices = [[idx for idx, val in enumerate(classified) if zone + 1 == val] for zone in range(maxz)]\n",
    "\n",
    "    # sum of squared deviations from array mean\n",
    "    sdam = np.sum((array - array.mean()) ** 2)\n",
    "\n",
    "    # sorted polygon stats\n",
    "    array_sort = [np.array([array[index] for index in zone]) for zone in zone_indices]\n",
    "\n",
    "    # sum of squared deviations of class means\n",
    "    sdcm = sum([np.sum((classified - classified.mean()) ** 2) for classified in array_sort])\n",
    "\n",
    "    # goodness of variance fit\n",
    "    gvf = (sdam - sdcm) / sdam\n",
    "\n",
    "    return gvf\n",
    "\n",
    "\n",
    "def classify(value, breaks):\n",
    "    for i in range(1, len(breaks)):\n",
    "        if value < breaks[i]:\n",
    "            return i\n",
    "    return len(breaks) - 1\n",
    "\n",
    "\n",
    "\n",
    "fpath='/Users/paulparis/Documents/Projects/csi/data/table/'\n",
    "fname = 'ETOPO1_10ksamplepts.csv'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# load data to array\n",
    "d=LoadSrcDataToArray(fpath, fname)\n",
    "\n",
    "\n",
    "# from the z vector, extract 10000 random observations w/replacement from d, and do it a 1000\n",
    "# times:\n",
    "\n",
    "breaksList=[]\n",
    "\n",
    "for i in range(0,1000,1):\n",
    "    s=[]\n",
    "    klasses=2\n",
    "    gvf = 0.0\n",
    "    gvf_acceptance=0.80\n",
    "    \n",
    "    for j in range(0,10000,1):\n",
    "        s.append(random.choice(d))\n",
    "        pc_array = np.array(s)\n",
    "    \n",
    "    # compute F-J natural breaks and goodness of fit:\n",
    "    while(gvf < gvf_acceptance):\n",
    "        #print('Trying', klasses, 'class breaks...')\n",
    "        # classify data using PySAL Natural Breaks Lib.\n",
    "        breaks=ComputeNaturalBreaks(pc_array, klasses)\n",
    "\n",
    "        # compute goodness of variance fit\n",
    "        gvf = goodness_of_variance_fit(pc_array, breaks.bins)   #(data, nclasses)\n",
    "        klasses+=1\n",
    "    \n",
    "    #print breaks\n",
    "    breaksList.append(breaks.bins)  \n",
    "\n",
    "print('Done')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-71.301351333\n",
      "-202.584322746\n",
      "-421.919413941\n",
      "-741.37677825\n"
     ]
    }
   ],
   "source": [
    "c1=0\n",
    "c2=0\n",
    "c3=0\n",
    "c4=0\n",
    "\n",
    "for i, a in enumerate(breaksList):\n",
    "    c4 += a[0]\n",
    "    c3 += a[1]\n",
    "    c2 += a[2]\n",
    "    c1 += a[3]\n",
    "\n",
    "print(c1/(i+1) )\n",
    "print(c2/(i+1) )\n",
    "print(c3/(i+1) )\n",
    "print(c4/(i+1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-71.301351333 <br>\n",
    "-202.584322746 <br>\n",
    "-421.919413941 <br>\n",
    "-741.37677825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
