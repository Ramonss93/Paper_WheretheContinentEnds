{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the processes associated with taking a random sample of portions of the global continental shelf as represented in the new shelf bathymetry model presented as part of this research. The random sample consists of p sample points, where p is a number probably greater than or equal to 100. From the points, sampling areas are generated. These areas, which at this writing consist of p 200 km diameter circles, will then be interrogated to collect location, depth, slope, and shelf area contained. These data, collected, can then be normalized, and inserted into an analysis workflow such as regression, principal components, or Q-mode factor analysis.\n",
    "\n",
    "Let's begin..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0:) Import the requisite modules and libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pyproj import Proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) **Set up the GRASS working environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to set up an external GRASS environment. This consists largely of setting a few environment variables and then instantiating the GRASS scripting class. Doing this permits us to call any of the GRASS libraries (sans the display code, I believe) from outside of a GRASS environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************\n",
      "Setting up GRASS Environment\n",
      "{'MAPSET': 'user', 'GISDBASE': '/Users/paulparis/Documents/Projects/csi/data/GRASSData', 'LOCATION_NAME': 'csi_shelf_mapping_global'}\n"
     ]
    }
   ],
   "source": [
    "# set the location and mapset for the GRASS instance to be created\n",
    "\n",
    "gisdbase = '/Users/paulparis/Documents/Projects/csi/data/GRASSData'    # location of your GRASS database\n",
    "location = 'csi_shelf_mapping_global'                                  # name of geographic Location\n",
    "mapset = 'user'         \n",
    "\n",
    "\n",
    "# This CLASS definition is used to initialize an instance of GRASS 7.x in this domain space\n",
    "class GrassBASE:\n",
    "    def initGRASSEnv( self, database, loc, map ):\n",
    "        gisbase=os.environ[ 'GISBASE' ] = '/Applications/GRASS-7.0.app/Contents/MacOS'\n",
    "        gisdbase=database\n",
    "        location=loc\n",
    "        mapset=map\n",
    "        sys.path.append(os.path.join(os.environ['GISBASE'], \"etc\", \"python\"))\n",
    "        import grass.script as grass\n",
    "        import grass.script.setup as gsetup\n",
    "        gsetup.init( gisbase, gisdbase, location, mapset )\n",
    "        print(grass.gisenv())\n",
    "        \n",
    "\n",
    "        \n",
    "# initiate a GRASS instance/environment\n",
    "\n",
    "print('')\n",
    "print ('********************************************')\n",
    "print ('Setting up GRASS Environment')\n",
    "g = GrassBASE()\n",
    "g.initGRASSEnv( gisdbase, location, mapset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A COMPOSITE CONTINENTAL SHELF **GEOMORPHON** FROM THE COMPONENT CLASSES (e.g., shallow, shallow intermediate,...)\n",
    "\n",
    "NOTE THAT **YOU NEED ONLY DO THIS ONCE. IF THE RASTER ALREADY EXISTS, NO NEED TO RECREATE!**\n",
    "\n",
    "in GRASS:\n",
    "    r.patch --o input=ETOPO1_geomorph_shallowshelf_1km,ETOPO1_geomorph_shallowintershelf_1km,ETOPO1_geomorph_deepintershelf_1km,ETOPO1_geomorph_deepshelf_1km output=ETOPO1_geomorph_compositeshelf_1km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A COMPOSITE CONTINENTAL SHELF (**BATHY** MODEL DEM) FROM THE GEOMORPHON COMPOSITE SHELF MODEL. WE'LL TAKE A RANDOM SAMPLE ACROSS THIS DEM NEXT...\n",
    "\n",
    "NOTE THAT **YOU NEED ONLY DO THIS ONCE. IF THE RASTER ALREADY EXISTS, NO NEED TO RECREATE!**\n",
    "\n",
    "In GRASS:\n",
    "r.mapcalc --o\n",
    "ETOPO1_bathy_compositeshelf_1km=if(ETOPO1_geomorph_compositeshelf_1km, ETOPO1_bathy_1km, null() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A COMPOSITE CONTINENTAL SHELF **SLOPE** MODEL FROM THE BATHY COMPOSITE SHELF MODEL GENERATED ABOVE. WE'LL SAMPLE ACROSS THIS DEM A BIT LATER...\n",
    "\n",
    "NOTE THAT **YOU NEED ONLY DO THIS ONCE. IF THE RASTER ALREADY EXISTS, NO NEED TO RECREATE!**\n",
    "\n",
    "In GRASS:\n",
    "r.slope.aspect --o elevation=ETOPO1_bathy_compositeshelf_1km slope=ETOPO1_slope_compositeshelf_1km "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.) Extract a p point random sample from the ETOPO1_geomorph_compositeshelf_1km**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the computational region...\n",
    "import grass.script as grass\n",
    "\n",
    "grass.run_command( 'g.region', flags='p', region='ETOPO1_World_1km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,) randomly sampled points generated.\n"
     ]
    }
   ],
   "source": [
    "import grass.script as grass\n",
    "\n",
    "# set number of random sample points, p, to collect...\n",
    "p=100000,  #250\n",
    "\n",
    "inp='ETOPO1_shelfslope_1211m_1km'\n",
    "vout='ETOPO1_100kshelfsamplepts'\n",
    "# extract the random sample of p points...\n",
    "grass.run_command( 'r.random', overwrite=True, input=inp, npoints=p, vector=vout)\n",
    "\n",
    "print p,'randomly sampled points generated.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## export vector data from GRASS to an external ASCII text file\n",
    "\n",
    "xy = grass.read_command('v.out.ascii', overwrite=True, input='ETOPO1_100kshelfsamplepts', output='/Users/paulparis/Documents/Projects/csi/data/vector/ETOPO1_100kshelfsamplepts.csv', separator=',', type='point',columns='*' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding some additional coverage to ensure that areas where shelves are sparsely represented get some representation in the analysis.**\n",
    "\n",
    "As there is a great deal of variation in the distribution of c. shelf across the globe--that is, some regions have broad extensive shelves, while others have little or no shelf to speak of--we've elected to edit the random sample points by hand so as to include some of those lesser well represented areas. Some of the original points will be altered. Some areas received, by chance, a large number of points which resulted in much overlap between sampling areas. Those added will be along the sparsely shelved western North and South American continents, as well as the east and west coasts of Africa, and Asia along its Indian Ocean exposure. Those deleted will come from anywhere where there are clusters of points which result in multiple sampling area overlaps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**3.) create buffer regions (sampling polygons) around the p random sample points...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import grass.script as grass\n",
    "\n",
    "d=125000   # buffer distance in map units (meters, probably)\n",
    "grass.run_command('v.buffer', flags='t', overwrite=True, input='ETOPO1_cshelfsamplepts', type='point', output='ETOPO1_cshelfsampleareas', distance=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# there are 281 sampling areas (p) to work...\n",
    "\n",
    "import grass.script as grass\n",
    "\n",
    "depth_formula = 'tmp_bathy=if(tmpsampleDEM,ETOPO1_bathy_compositeshelf_1km, null())'\n",
    "slope_formula = 'tmp_slope=if(tmpsampleDEM,ETOPO1_slope_compositeshelf_1km, null())'\n",
    "\n",
    "# ## open a text file to receive the northing (we'll convert to latitude later),depths, slopes,\n",
    "# ## and shelf area, all measured within each of the p 250km diameter study areas\n",
    "fp = open('/Users/paulparis/Documents/Projects/csi/data/vector/factor_analysis.csv','w')\n",
    "\n",
    "try:\n",
    "    for p in range(1,282,1):\n",
    "        #grass.run_command('v.extract', overwrite=True, input='ETOPO1_cshelfsampleareas', type='area', cats=p, output='tempsamplearea' )\n",
    "        grass.run_command( 'v.to.rast', overwrite=True, input='ETOPO1_cshelfsampleareas', cats=p, use='cat', output='tmpsampleDEM')\n",
    "        grass.mapcalc(  depth_formula, overwrite=True )\n",
    "        grass.mapcalc(  slope_formula, overwrite=True )\n",
    "\n",
    "        # ## collect the x,y position of the current samping area's centroid...\n",
    "        xy = grass.read_command('v.out.ascii', input='ETOPO1_cshelfsamplepts', type='point', cats=p )\n",
    "\n",
    "        # ## generate and collect basic statistics on depths, slopes, and location...\n",
    "        depths = grass.read_command( 'r.univar', flags='t', map='tmp_bathy', separator=',' )\n",
    "        slopes = grass.read_command( 'r.univar', flags='t', map='tmp_slope', separator=',' )\n",
    "        \n",
    "        if( len(depths) > 100):\n",
    "            # ## parse the xy, depths, and slopes data in prep for writing to an output file...\n",
    "            # ## for the xy location...\n",
    "            x = xy.rstrip().split('|')[0]\n",
    "            y = xy.rstrip().split('|')[1]\n",
    "        \n",
    "            # ## for the depths...\n",
    "            depth = depths.split(',')\n",
    "            n = depth[11].split('\\n')[1]  \n",
    "            d_max = float(depth[13])\n",
    "            d_mean = float(depth[16])\n",
    "            d_std = float(depth[18])\n",
    "        \n",
    "            # ## for the slopes...\n",
    "            slope = slopes.split(',')\n",
    "            #s_n = int(slope[11].split('\\n')[1])\n",
    "            #s_min = float(slope[13])\n",
    "            s_mean = float(slope[16])\n",
    "            s_std = float(slope[18])\n",
    "        \n",
    "            # un-project the x and y coordinates to GPs...\n",
    "            pdef = Proj(proj='igh',zone=10,ellps='WGS84')\n",
    "            lon,lat = pdef(x,y, inverse=True)\n",
    "            \n",
    "            #print x,y,n,d_max,d_mean,d_std,s_mean,s_std\n",
    "            fp.write(str(x)+','+str(y)+','+str(n)+','+str(d_max)+','+str(d_mean)+','+str(d_std)+','+str(s_mean)+','+str(s_std)+'\\n' )\n",
    "        else:\n",
    "            print 'skipping missing cat record:',p\n",
    "            \n",
    "except IndexError:\n",
    "    print 'missing area', p\n",
    "    \n",
    "fp.close()\n",
    "print 'Fin!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Re-read the site sampling data file generated above to set the northing values (column 2)\n",
    "# ## to their absolute values...we don't want values here < 0.\n",
    "\n",
    "\n",
    "ifn='/Users/paulparis/Documents/Projects/csi/data/vector/factor_analysis.csv'\n",
    "ofn='/Users/paulparis/Documents/Projects/csi/data/vector/factor_analysis2.csv'\n",
    "\n",
    "cols=['easting','northing','p','depth_max','depth_mean','depth_std','slope_mean','slope_std']\n",
    "\n",
    "idat = open(ifn, 'r')\n",
    "odat = open(ofn, 'w')\n",
    "\n",
    "for i, line in enumerate(idat):\n",
    "    _,northing,p,depth_max,depth_mean,depth_std,slope_mean,slope_std=line.rstrip().split(',')\n",
    "    north=str(int(abs(float(northing))))\n",
    "    print i,north,p\n",
    "    odat.write(north+','+p+','+depth_max+','+depth_mean+','+depth_std+','+slope_mean+','+slope_std+'\\n')\n",
    "\n",
    "print 'Fin!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5222129\n"
     ]
    }
   ],
   "source": [
    "print int(5222129.82357993)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
